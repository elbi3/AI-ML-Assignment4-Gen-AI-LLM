{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470c750d-ff8b-45f2-859e-e882bfbf32a2",
   "metadata": {},
   "source": [
    "# GEN AI Q&A with T5 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfee37a-abb6-49d6-aed6-3d33250bd992",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802d83d8-1881-4fab-aa2e-3d67b287eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n",
      "GPU Available:  []\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "# install backend (deep learning framework that does the heavy tensor math)\n",
    "import tensorflow as tf\n",
    "# import tf_keras as keras\n",
    "print(tf.__version__)\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU')) #check this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3842b46f-aa42-4b7c-99e1-893466ecefae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a916d8cd-c728-4270-8388-fdde802fb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install transformers (high-level frontend API to work with models)\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration #tensorflow's hugging face t5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516178e-9df0-4e6c-a852-ddfa1ccc72ae",
   "metadata": {},
   "source": [
    "## 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e310c3fe-293c-47f1-968e-f463a18613b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: google/flan-t5-small\n",
      "Running on: CPU\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG CELL ===\n",
    "CONFIG = {\n",
    "    \"model_name\": \"google/flan-t5-small\", # swap out with base or large later\n",
    "    \"device\": \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\",\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Using model: {CONFIG['model_name']}\")\n",
    "print(f\"Running on: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e812aa-9cbb-4a3a-902a-94614bdbbae9",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cedaa9f0-0165-4b8d-a1e1-47093a336abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtins.safe_open' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. load pre-trained model and tokenizer and reference CONFIG from step 2:\u001b[39;00m\n\u001b[32m      2\u001b[39m tokenizer = T5Tokenizer.from_pretrained(CONFIG[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m]) \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mTFT5ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\T5-LLM-Project\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:2964\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework=\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m safetensors_archive:\n\u001b[32m   2961\u001b[39m         \u001b[38;5;66;03m# Load from a PyTorch safetensors checkpoint\u001b[39;00m\n\u001b[32m   2962\u001b[39m         \u001b[38;5;66;03m# We load in TF format here because PT weights often need to be transposed, and this is much\u001b[39;00m\n\u001b[32m   2963\u001b[39m         \u001b[38;5;66;03m# faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2964\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_state_dict_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m            \u001b[49m\u001b[43msafetensors_archive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to build the model again\u001b[39;49;00m\n\u001b[32m   2968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m safetensors_from_pt:\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_sharded_pytorch_safetensors_in_tf2_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\T5-LLM-Project\\Lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:333\u001b[39m, in \u001b[36mload_pytorch_state_dict_in_tf2_model\u001b[39m\u001b[34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Convert old format to new format if needed from a PyTorch state_dict\u001b[39;00m\n\u001b[32m    332\u001b[39m tf_keys_to_pt_keys = {}\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'builtins.safe_open' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 1. load pre-trained model and tokenizer and reference CONFIG from step 2:\n",
    "tokenizer = T5Tokenizer.from_pretrained(CONFIG[\"model_name\"]) \n",
    "model = TFT5ForConditionalGeneration.from_pretrained(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c8ec2a-31e5-472d-8939-881fd773e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare \"question + context\" input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f89004-3448-4fac-a41c-17a396cd2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e76eaf-3831-4d40-85a0-8caeb5a137ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t5_project)",
   "language": "python",
   "name": "t5_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
